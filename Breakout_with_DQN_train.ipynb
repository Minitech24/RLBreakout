{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breakout_with_DQN_train",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I99WS90k3aPM"
      },
      "source": [
        "**Breakout with DQN - training the model**\n",
        "\n",
        "Team: Happy Campers\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra5MevXgYSif"
      },
      "source": [
        "#Install the dependencies\n",
        "#!pip install gym\n",
        "#!apt-get install python-opengl -y\n",
        "#!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkMeuqHsaIM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85baa7ac-76a6-4c22-ccb5-1076d7c4366d"
      },
      "source": [
        "#importing libraries\n",
        "\n",
        "%matplotlib inline\n",
        "from gym import wrappers\n",
        "from time import sleep\n",
        "import time\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# interacting with GoogleDrive\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhWXThwca31G"
      },
      "source": [
        "# DQN from the google deep mind paper\n",
        "class DQN(nn.Module): #extends nn. module\n",
        "    def __init__(self, img_height, img_width, num_frames=4): #screenshot-like images as input\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4) #four hidden convolutional layers and an output layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.dense = torch.nn.Linear(64 * 7 * 7, 512)\n",
        "        self.out = nn.Linear(512, 4)   # four outputs - our action space       \n",
        "\n",
        "\n",
        "  # implements a forward pass through the network     \n",
        "    def forward(self, t):\n",
        "        t = F.relu(self.conv1(t))\n",
        "        t = F.relu(self.conv2(t))\n",
        "        t = F.relu(self.conv3(t))\n",
        "        t = t.view(t.size(0), -1)\n",
        "        t = F.relu(self.dense(t))\n",
        "        t = self.out(t)\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWia6olXa4fN"
      },
      "source": [
        "#class experience that creates instances of experience objects that will get stored in replay memory\n",
        "Experience = namedtuple(\n",
        "    'Experience',\n",
        "    ('state', 'action', 'next_state', 'reward')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajUJdLIia6ab"
      },
      "source": [
        "# stores experiences and provides samples of the experiences\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = [] #holds stored experiences\n",
        "        self.push_count = 0 #how many experiences we've added\n",
        "\n",
        "    # pushes new experience to memory \n",
        "    def push(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count % self.capacity] = experience #we push new experiences, replacing the oldest ones first\n",
        "        self.push_count += 1\n",
        "\n",
        "    # samples the number of experiences from memory that match the batch size\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    #we can only provide a sample if the number of experiences is higher or equal to batch size\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfMK1ALQa8Eq"
      },
      "source": [
        "# implementatio of the EpsilonGreedy strategy\n",
        "class EpsilonGreedyStrategy():\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "    \n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * \\\n",
        "            math.exp(-1. * current_step * self.decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ugf2J4a-MB"
      },
      "source": [
        "# implementation of the agent\n",
        "class Agent():\n",
        "    def __init__(self, strategy, num_actions, device): # device we use for tensor calculations\n",
        "        self.current_step = 0\n",
        "        self.strategy = strategy\n",
        "        self.num_actions = num_actions\n",
        "        self.device = device\n",
        "\n",
        "    def select_action(self, state, policy_net): # selecting the next action\n",
        "        rate = strategy.get_exploration_rate(self.current_step)\n",
        "        self.current_step += 1\n",
        "\n",
        "        if rate > random.random():\n",
        "            action = random.randrange(self.num_actions)\n",
        "            return torch.tensor([action]).to(self.device) # explore      \n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).argmax(dim=1).to(self.device) # exploit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OqovxusbATS"
      },
      "source": [
        "#plot intermediate results while training\n",
        "def plot(values, moving_avg_period, time_taken):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Points')\n",
        "    plt.plot(values)\n",
        "    \n",
        "    moving_avg = get_moving_average(moving_avg_period, values)\n",
        "    plt.plot(moving_avg)    \n",
        "    plt.pause(0.001)\n",
        "    print(\"Episode\", len(values), \"\\n\", \\\n",
        "          moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
        "    print(f\"Episode times: {time_taken[-1]} (s)\")\n",
        "    if is_ipython: display.clear_output(wait=True)\n",
        "\n",
        "# function for calculating the moving average\n",
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
        "            .mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdz7wRNRbCs7"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "\n",
        "    t1 = torch.cat(batch.state)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.reward)\n",
        "    t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_kh4ALxbFfU"
      },
      "source": [
        "class QValues():\n",
        "  # we create a device since we won't be creating an instance of this class\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # two static methods we use to get our current and maximum q values for next states\n",
        "    @staticmethod\n",
        "    def get_current(policy_net, states, actions): \n",
        "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1)) #pytorch tensor\n",
        "    \n",
        "    @staticmethod        \n",
        "    def get_next(target_net, next_states):                \n",
        "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
        "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
        "        non_final_state_locations = (final_state_locations == False)\n",
        "        non_final_states = next_states[non_final_state_locations]\n",
        "        batch_size = next_states.shape[0]\n",
        "        values = torch.zeros(batch_size).to(QValues.device)\n",
        "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "        return values #maxim q values for next states, returns pytorch tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEKyPS-IbH2g"
      },
      "source": [
        "# we encapsulate the environment functionalities in the environment manager and define additional functions\n",
        "class BreakoutEnvManager():\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.env = gym.make('BreakoutNoFrameskip-v4').unwrapped\n",
        "        self.env = wrappers.AtariPreprocessing(self.env)\n",
        "        self.env = wrappers.FrameStack(self.env, num_stack = 4)\n",
        "        self.env.reset()\n",
        "        self.current_screen = None\n",
        "        self.done = False\n",
        "    \n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        self.current_screen = None\n",
        "        \n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "        \n",
        "    def render(self, mode='human'):\n",
        "        return self.env.render(mode)\n",
        "        \n",
        "    def num_actions_available(self):\n",
        "        return self.env.action_space.n\n",
        "        \n",
        "    def take_action(self, action):        \n",
        "        _, reward, self.done, _ = self.env.step(action.item())\n",
        "        return torch.tensor([reward], device=self.device)\n",
        "    \n",
        "\n",
        "    def get_state(self):\n",
        "        self.current_screen = self.get_processed_screen()\n",
        "        return self.current_screen\n",
        "\n",
        "    \n",
        "    def get_screen_height(self):\n",
        "        screen = self.get_processed_screen()\n",
        "        return screen.shape[2]\n",
        "    \n",
        "    def get_screen_width(self):\n",
        "        screen = self.get_processed_screen()\n",
        "        return screen.shape[3]\n",
        "       \n",
        "    def get_processed_screen(self):\n",
        "        screen = np.asarray(self.env.frames) # PyTorch expects CHW\n",
        "        return self.transform_screen_data(screen)\n",
        "    \n",
        "    def transform_screen_data(self, screen):       \n",
        "        # Convert to float, rescale, convert to tensor\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        \n",
        "        return screen.unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBX2BkE9VC1M"
      },
      "source": [
        "# function that periodically saves the model to Google Drive - a models folder must be created in the Drive beforehand\n",
        "def save_state(episode, policy_net, target_net, optimizer, agent, configuration):\n",
        "    state = {\n",
        "      'episode': episode,\n",
        "      'policy_state_dict': policy_net.state_dict(),\n",
        "      'target_state_dict': target_net.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'agent_current_step': agent.current_step\n",
        "    }\n",
        "\n",
        "    model_save_name = f'classifier_config_{configuration}.pt'\n",
        "    filepath = f\"/content/gdrive/My Drive/models/{model_save_name}\" \n",
        "    torch.save(state, filepath)\n",
        "\n",
        "    \n",
        "    model_save_name = f'classifier_{episode}_config_{configuration}.pt'\n",
        "    filepath = f\"/content/gdrive/My Drive/models/{model_save_name}\" \n",
        "    torch.save(state, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewmLZl2pbKSM"
      },
      "source": [
        "# configuration - different configurations used are in the write up\n",
        "batch_size = 256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update = 10\n",
        "memory_size = 100000\n",
        "lr = 0.00001\n",
        "num_episodes = 12000\n",
        "configuration = 0 # We have different configurations of parameters - the variable serves for saving the model configurations = {0,1,2,3,4,5,6}\n",
        "\n",
        "last_ep_cp = 0\n",
        "\n",
        "#Load model - since the models are periodically saved, they can be uploaded and trained from that episode onwards\n",
        "# to load a model, all lines with a \"#\" at the end must be uncommented\n",
        "#last_ep_cp = 0 # the num. of episodes the uploaded model has been trained for so far #\n",
        "#model_save_name = f'classifier_{last_ep_cp}_config_{configuration}.pt' #\n",
        "#path = f\"/content/gdrive/My Drive/models/{model_save_name}\"  #\n",
        "#checkpoint = torch.load(path)  #\n",
        "\n",
        "#we run on a GPU, as it is about 40 times faster than the cpu. Thank you Google!\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "#define the environment manager\n",
        "em = BreakoutEnvManager(device)\n",
        "\n",
        "\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "#agent gives you the proper epsilon value\n",
        "agent = Agent(strategy, em.num_actions_available(), device)\n",
        "\n",
        "#the current step is loaded from the loaded model\n",
        "#agent.current_step = checkpoint['agent_current_step'] #\n",
        "\n",
        "memory = ReplayMemory(memory_size)\n",
        "\n",
        "\n",
        "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
        "\n",
        "# the policy net state is loaded from the loaded model\n",
        "#policy_net.load_state_dict(checkpoint['policy_state_dict']) #\n",
        "\n",
        "policy_net.eval()\n",
        "\n",
        "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
        "\n",
        "# the target net state is loaded from the loaded model\n",
        "#target_net.load_state_dict(checkpoint['target_state_dict']) #\n",
        "\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "\n",
        "# the optimizer from the loaded model is loaded\n",
        "#optimizer.load_state_dict(checkpoint['optimizer']) #\n",
        "\n",
        "episode_points = []\n",
        "episode_times = []\n",
        "for episode in range(last_ep_cp+1, last_ep_cp+num_episodes):\n",
        "\n",
        "    # the model is saved every 500 episodes\n",
        "    if episode % 500 == 0:\n",
        "      save_state(episode, policy_net, target_net, optimizer, agent, configuration)\n",
        "\n",
        "    start_time = time.time()\n",
        "    em.reset()\n",
        "    total_reward = 0\n",
        "    state = em.get_state()\n",
        "\n",
        "    for timestep in count():\n",
        "\n",
        "        #select and perform action, record reward and next state\n",
        "        action = agent.select_action(state, policy_net)\n",
        "        reward = em.take_action(action)\n",
        "        total_reward += reward\n",
        "        next_state = em.get_state()\n",
        "        #experience is pushed to memory\n",
        "        memory.push(Experience(state, action, next_state, reward))\n",
        "        state = next_state\n",
        "\n",
        "        # minibatches\n",
        "        if memory.can_provide_sample(batch_size):\n",
        "            experiences = memory.sample(batch_size)\n",
        "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "            \n",
        "            # estimate current and max future Q values\n",
        "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
        "            next_q_values = QValues.get_next(target_net, next_states)\n",
        "            target_q_values = (next_q_values * gamma) + rewards\n",
        "\n",
        "            # calculate loss and do a backward pass through the net\n",
        "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        if em.done:\n",
        "            episode_points.append(total_reward)\n",
        "\n",
        "            end_time = time.time()\n",
        "            time_elapsed = end_time - start_time\n",
        "            episode_times.append(time_elapsed)\n",
        "\n",
        "            #plot progress\n",
        "            plot(episode_points, 100, episode_times)\n",
        "            break\n",
        "\n",
        "    # every 10th episode, update the target net with the current policy net\n",
        "    if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        \n",
        "em.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}