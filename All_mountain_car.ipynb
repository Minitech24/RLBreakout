{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "All mountain car.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQpv0FZnX09r"
      },
      "source": [
        "### Mountain car - all configurations\n",
        "\n",
        "#### Team: Happy Campers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "C1vbWCRRX09s"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Random play when epsilon = 1 and epsilon decay are commented out (rows 90-93)\n"
        },
        "id": "TNyuVBTvX09t"
      },
      "source": [
        "#Configuration 1 with the random agent\n",
        "\n",
        "# Learning rate = .1, epsilon = 1,\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UqnNhmsAX09v"
      },
      "source": [
        "#Configuration 1 with the greedy agent\n",
        "\n",
        "#learning rate = .1, epsilon = .5\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .5  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t0KGzIEAX09y"
      },
      "source": [
        "#Plot for configuration 1\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1) vs Greedy Player (Epsilon = .5), LR .1\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vttdW4ILX09y"
      },
      "source": [
        "#Configuration 2 - random agent\n",
        "#Learning rate = .5, epsilon = 1\n",
        "\n",
        "LEARNING_RATE = 0.5\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OwZfLgLHX091"
      },
      "source": [
        "#Configuration - greedy player\n",
        "\n",
        "#Learning rate = .5, epsilon = .5\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.5\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .5  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SpdM0fApX092"
      },
      "source": [
        "#Configuration 2 plot\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1)  vs Greedy Player (Epsilon .5), LR .5\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IBA4gf7jX092"
      },
      "source": [
        "#Configuration 3 - random player\n",
        "\n",
        "# Learning rate = .1, epsilon = 1\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CpIgGNTcX093"
      },
      "source": [
        "#Configuration 3 - greedy player\n",
        "\n",
        "# Learning rate = .1, epsilon = .1\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "n8jXhGD7X094"
      },
      "source": [
        "#Configuration 3 plot\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1) vs Greedy Player (Epsilon .1) LR .1\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3jImGzxsX094"
      },
      "source": [
        "#Configuration 4 - random player\n",
        "\n",
        "# Learning rate = .1, epsilon = 1\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eMqopYemX095"
      },
      "source": [
        "#Configuration 4 - greedy player\n",
        "\n",
        "# Learning rate = .1, epsilon = .01\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .01  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J-Zz7j1YX096"
      },
      "source": [
        "#Configuration 4 plot\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1) vs Greedy Player (Epsilon .01) LR .1\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OneUwfbxX096"
      },
      "source": [
        "#Configuration 5 - random player\n",
        "\n",
        "# Learning rate = .01, epsilon = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cquSYN6dX097"
      },
      "source": [
        "#Configuration 5 - greedy player\n",
        "\n",
        "# Learning rate = .01, epsilon = .01\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .01  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qRXbesFLX097"
      },
      "source": [
        "#Configuration 5 plot\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1) vs Greedy Player (Epsilon .01) LR .01\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k6OcIRP8X098"
      },
      "source": [
        "#Configuration 6 - random player\n",
        "\n",
        "# Learning rate = .01, epsilon = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Rtyabq8NX098"
      },
      "source": [
        "#Configuration 6 - greedy agent\n",
        "\n",
        "# Learning rate = .1, epsilon = .001\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .001  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    #if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "     #   epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p3nJIqI2X09-"
      },
      "source": [
        "#Configuration 6 plot\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent (Epsilon = 1) vs Greedy Player (Epsilon .001), LR .1\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hptzIl5YX09-"
      },
      "source": [
        "#Configuration 7 - random agent\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "# Learning rate = .1, epsilon = 1, adding epsilon decay\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  #Exploration/exploitation parameter. When equal to 1 agent plays randomly\n",
        "#START_EPSILON_DECAYING = 1\n",
        "#END_EPSILON_DECAYING = EPISODES//2\n",
        "#epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "rand_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    # Comment out to avoid epsilon decay\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        rand_aggr_ep_rewards['ep'].append(episode)\n",
        "        rand_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        rand_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        rand_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"avg\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['min'], label = \"min\")\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['max'], label = \"max\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J6d0sUjKX09_"
      },
      "source": [
        "#Configuration 7 - greedy player\n",
        "\n",
        "# Learning rate = .1, epsilon = .01\n",
        "\n",
        "# When grey out decay, epsilon is fixed which means playing epsilon greedy\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 10000\n",
        "SHOW_EVERY = 500\n",
        "STATS_EVERY = 100\n",
        "\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = .01  #Exploration/exploitation parameter. When equal to 1 agent plays randomly (only explores)\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES//2\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "# payoff = 0 is the payoff when car reaches flag\n",
        "# initializes 20x20x3 table which is the observation space (20x20) for all three actions\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [] #contains each episodes reward as a list\n",
        "greedy_aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []} #dictionary that tracks episode number, average, min, max\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(episode)\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table. Change to random to to random action (action space = 3 here. print(env.action_space.n) to print action space)\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        # Could add print statement to get printout of all the states\n",
        "        episode_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "        # If simulation did not end yet after last step - update Q table\n",
        "        if not done:\n",
        "\n",
        "            # Maximum possible Q value in next step (for new state)\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            # Current Q value (for current state and performed action)\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            # Equation for a new Q value for current state and action\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            # Update Q table with new Q value\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "\n",
        "        # Simulation ended (for any reason) - if goal position is archived - update Q value with reward directly\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            #q_table[discrete_state + (action,)] = reward\n",
        "            q_table[discrete_state + (action,)] = 0\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % SHOW_EVERY:\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
        "        greedy_aggr_ep_rewards['ep'].append(episode)\n",
        "        greedy_aggr_ep_rewards['avg'].append(average_reward)\n",
        "        greedy_aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "        greedy_aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zpg3xTjlX09_"
      },
      "source": [
        "#Configuration 7 plot\n",
        "\n",
        "plt.plot(rand_aggr_ep_rewards['ep'], rand_aggr_ep_rewards['avg'], label = \"random avg\")\n",
        "plt.plot(greedy_aggr_ep_rewards['ep'], greedy_aggr_ep_rewards['avg'], label = \"greedy avg\")\n",
        "plt.axis('on')\n",
        "plt.title(\"Random Agent vs Greedy, LR .1, and Epsilon Decay\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}